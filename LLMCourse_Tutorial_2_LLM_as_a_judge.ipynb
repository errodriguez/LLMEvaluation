{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tdvm881pyC8w"
   },
   "source": [
    "# Installation and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "thDJOro8nijJ"
   },
   "outputs": [],
   "source": [
    "!pip install evidently[llm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dIVmb7pBxyHp"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from evidently import Dataset\n",
    "from evidently import DataDefinition\n",
    "from evidently import Report\n",
    "from evidently import BinaryClassification\n",
    "from evidently.descriptors import *\n",
    "from evidently.presets import TextEvals, ValueStats, ClassificationPreset, DataSummaryPreset\n",
    "from evidently.metrics import *\n",
    "\n",
    "from evidently.llm.templates import BinaryClassificationPromptTemplate\n",
    "\n",
    "from evidently.sdk.models import PanelMetric\n",
    "from evidently.sdk.panels import DashboardPanelPlot\n",
    "\n",
    "from evidently.ui.workspace import CloudWorkspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evPnRvlIdX8O"
   },
   "source": [
    "In this tutorial, we will:\n",
    "- Define the evaluation criteria for our LLM judge\n",
    "- Build an LLM-as-a-Judge using different prompts/models\n",
    "- Evaluate the quality of the judge comparing results to human labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Tzr4GcygLNr"
   },
   "source": [
    "# (Optional) Set up Evidently Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ZbeRCC-9tKV"
   },
   "source": [
    "Set up API keys for LLM judges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5jVf-0TJQ6C"
   },
   "outputs": [],
   "source": [
    "## import os\n",
    "## os.environ[\"OPENAI_API_KEY\"] = \"OPEN_AI_API_KEY\"\n",
    "## os.environ[\"ANTHROPIC_API_KEY\"] = \"ANTHROPIC_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WdDmKNmghNP"
   },
   "source": [
    "**Optional**. Connect to Cloud and create a Project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfNSnYQTgK4z"
   },
   "outputs": [],
   "source": [
    "# ws = CloudWorkspace(token=\"YOUR_API_TOKEN\", url=\"https://app.evidently.cloud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qv9EblcaxDpa"
   },
   "outputs": [],
   "source": [
    "#project = ws.create_project(\"My project name\", org_id=\"YOUR_ORG_ID\")\n",
    "#project.description = \"My project description\"\n",
    "\n",
    "# or project = ws.get_project(\"PROJECT_ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNFjnBiN382e"
   },
   "source": [
    "# Prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noZ_CNMd4CCY"
   },
   "source": [
    "We start with an expert-labeled dataset. We will use it as the ground truth for our LLM judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uVu_RLf026mk"
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/evidentlyai/community-examples/main/datasets/code_review_dataset.csv\"\n",
    "review_dataset = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlEq15vC3EpU"
   },
   "source": [
    "Preview:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4NnMJHK6y0r2"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "review_dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyQBJx0P-YtI"
   },
   "source": [
    "Create an Evidently dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TxS43-M7GOEQ"
   },
   "outputs": [],
   "source": [
    "definition = DataDefinition(\n",
    "    text_columns=[\"Generated review\", \"Expert comment\"],\n",
    "    categorical_columns=[\"Expert label\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmp8HGvvGEen"
   },
   "outputs": [],
   "source": [
    "eval_dataset = Dataset.from_pandas(\n",
    "    pd.DataFrame(review_dataset),\n",
    "    data_definition=definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYTDRwu9I3PG"
   },
   "source": [
    "Preview the distribution of classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yUZkYba1Gv7N"
   },
   "outputs": [],
   "source": [
    "report = Report([\n",
    "  ValueStats(column=\"Expert label\")\n",
    "])\n",
    "\n",
    "my_eval = report.run(eval_dataset)\n",
    "my_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HveSNA6K07PX"
   },
   "source": [
    "**Optional**. Let's upload the source dataset to Evidently Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jEg5Mlgh1Cgt"
   },
   "outputs": [],
   "source": [
    "ws.add_dataset(\n",
    "    dataset = eval_dataset,\n",
    "    name = \"source_dataset\",\n",
    "    project_id = project.id,\n",
    "    description = \"Dataset with expert labels on review quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUZ8DoH609KB"
   },
   "source": [
    "# Our goal: create LLM judge to match the human labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIpXFb9MMtwb"
   },
   "source": [
    "**Options**:\n",
    "- Splitting criteria: (actionable / non-actionable, appropriate tone / inappropriate tone).\n",
    "- Try create a good/bad judge. (It may be useful to introduce a borderline or \"needs review\" tag for subtle or new cases)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uifPvCmI34sj"
   },
   "source": [
    "# Exp 1. Design the LLM judge - First try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3937d3EseMUR"
   },
   "source": [
    "For the tutorial flow, we'll keep the steps explicit and run 5 sequential experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3zPanZttrRX"
   },
   "source": [
    "First attempt to create the judge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BMnM3wvjiHUu"
   },
   "outputs": [],
   "source": [
    "# 1. Name the experiment\n",
    "name = \"naive_prompt\"\n",
    "\n",
    "# 2. Define LLM judge prompt template\n",
    "feedback_quality = BinaryClassificationPromptTemplate(\n",
    "        pre_messages=[(\"system\", \"You are evaluating the quality of code reviews given to junior developers.\")],\n",
    "        criteria = \"\"\"An review is GOOD when it's actionable and constructive.\n",
    "        A review is BAD when is non-actionable or overly critical.\n",
    "        \"\"\",\n",
    "        target_category=\"bad\",\n",
    "        non_target_category=\"good\",\n",
    "        uncertainty=\"unknown\",\n",
    "        include_reasoning=True,\n",
    "        )\n",
    "\n",
    "# 3. Apply the LLM judge\n",
    "eval_dataset = Dataset.from_pandas(\n",
    "    pd.DataFrame(review_dataset),\n",
    "    data_definition=definition,\n",
    "    descriptors=[\n",
    "        LLMEval(\"Generated review\",\n",
    "                template=feedback_quality,  # We can pass new prompt version\n",
    "                provider=\"openai\",          # We can change the provider\n",
    "                model=\"gpt-4o-mini\",        # We can change the model\n",
    "                alias=\"LLM-judged quality\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 4. Add TRUE/FALSE for judge alignment\n",
    "eval_dataset.add_descriptors([\n",
    "    ExactMatch(columns=[\"LLM-judged quality\", \"Expert label\"], alias=\"Judge_alignment\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cJEEClFvBW4c"
   },
   "outputs": [],
   "source": [
    "#print(feedback_quality.get_template())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBIT1vvti8jl"
   },
   "source": [
    "**LLM judgments**. View all results locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lsa6mxqTi-8k"
   },
   "outputs": [],
   "source": [
    "eval_dataset.as_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwQxc05EjUjU"
   },
   "source": [
    "**Report**. Let's summarize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ecGF6dv4jrV5"
   },
   "outputs": [],
   "source": [
    "report = Report([\n",
    "    TextEvals()\n",
    "])\n",
    "\n",
    "my_eval = report.run(eval_dataset)\n",
    "my_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9P5xYTeJj-7g"
   },
   "source": [
    "**Classification quality**. This function runs the Classification Report to evaluate the LLM judge quality and optionally uploads it to Evidently Cloud (if the workspace is set) with a tag.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hn5fpk42nNAR"
   },
   "outputs": [],
   "source": [
    "def run_classification_report(eval_dataset, name=None, cloud_ws=None, project_id=None):\n",
    "\n",
    "    df = eval_dataset.as_dataframe()\n",
    "    df_filtered = df[df[\"LLM-judged quality\"] != \"UNKNOWN\"]\n",
    "\n",
    "    # Set the classification Data Definition\n",
    "    definition_class = DataDefinition(\n",
    "        classification=[BinaryClassification(\n",
    "            target=\"Expert label\",\n",
    "            prediction_labels=\"LLM-judged quality\",\n",
    "            pos_label=\"bad\"\n",
    "        )],\n",
    "        categorical_columns=[\"Expert label\", \"LLM-judged quality\"]\n",
    "    )\n",
    "\n",
    "    # Create a Dataset object\n",
    "    eval_data = Dataset.from_pandas(df_filtered, data_definition=definition_class)\n",
    "\n",
    "    # Build classification report\n",
    "    report = Report([\n",
    "        ClassificationPreset(),\n",
    "        ValueStats(\"LLM-judged quality\"),\n",
    "        ValueStats(\"Expert label\")\n",
    "    ])\n",
    "\n",
    "    # Apply tag(s)\n",
    "    tags = [name] if name else []\n",
    "\n",
    "    my_eval = report.run(eval_data, tags=tags)\n",
    "\n",
    "    # Optional: upload to Evidently Cloud\n",
    "    if cloud_ws and project_id:\n",
    "        cloud_ws.add_run(project_id, my_eval, include_data=True)\n",
    "\n",
    "    return my_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wRnIRDZhpefV"
   },
   "source": [
    "(See all Evidently Metrics and Presets: https://docs.evidentlyai.com/metrics/all_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6cNjqvxoCkD"
   },
   "source": [
    "Run the function to evaluate the LLM judge quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ta9MOaHYpr2T"
   },
   "outputs": [],
   "source": [
    "my_eval = run_classification_report(\n",
    "    eval_dataset,\n",
    "    name=name,\n",
    "    cloud_ws=ws, #Optional\n",
    "    project_id=project.id #Optional\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKPozp0wD4Y1"
   },
   "source": [
    "You can also preview the classification report locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AFUMhtNlqeog"
   },
   "outputs": [],
   "source": [
    "my_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRl14wXsq5Yb"
   },
   "source": [
    "# (Optional) Add dashboard-as-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hywgxAwf0lwW"
   },
   "source": [
    "This will create a dashboard to track evaluation results over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwhlTUfDzdob"
   },
   "outputs": [],
   "source": [
    "project.dashboard.add_panel(\n",
    "             DashboardPanelPlot(\n",
    "                title=\"LLM judge quality\",\n",
    "                subtitle = \"Quality of the LLM judge that evaluates reviews compared to human labels.\",\n",
    "                size=\"full\",\n",
    "                values=[\n",
    "                    PanelMetric(\n",
    "                        metric=\"Precision\",\n",
    "                        legend=\"Precision\"\n",
    "                    ),\n",
    "                    PanelMetric(\n",
    "                        metric=\"Recall\",\n",
    "                        legend=\"Recall\"\n",
    "                    ),\n",
    "                    PanelMetric(\n",
    "                        metric=\"Accuracy\",\n",
    "                        legend=\"Accuracy\"\n",
    "                    ),\n",
    "                ],\n",
    "                plot_params={\"plot_type\": \"line\"},\n",
    "            ),\n",
    "            tab=\"LLM evals\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tzWBsYmNQ2Z"
   },
   "source": [
    "# Exp 2. Try another prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7d2yACaDuRJC"
   },
   "source": [
    "Let's try writing a more detailed prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mRVei9isytp"
   },
   "outputs": [],
   "source": [
    "# 1. Name the experiment <- new name\n",
    "name = \"detailed_prompt\"\n",
    "\n",
    "# 2. Define LLM judge prompt template  <- new prompt\n",
    "feedback_quality_2 = BinaryClassificationPromptTemplate(\n",
    "    pre_messages=[(\"system\", \"You are evaluating the quality of code reviews given to junior developers.\")],\n",
    "    criteria=\"\"\"\n",
    "    A review is **GOOD** if it is actionable and constructive. It should:\n",
    "    - Offer clear, specific suggestions or highlight issues in a way that the developer can address\n",
    "    - Be respectful and encourage learning or improvement\n",
    "    - Use professional, helpful language—even when pointing out problems\n",
    "\n",
    "    A review is **BAD** if it is non-actionable or overly critical. For example:\n",
    "    - It may be vague, generic, or hedged to the point of being unhelpful\n",
    "    - It may focus on praise only, without offering guidance\n",
    "    - It may sound dismissive, contradictory, harsh, or robotic\n",
    "    - It may raise a concern but fail to explain what should be done\n",
    "    \"\"\",\n",
    "    target_category=\"bad\",\n",
    "    non_target_category=\"good\",\n",
    "    uncertainty=\"unknown\",\n",
    "    include_reasoning=True,\n",
    ")\n",
    "\n",
    "# 3. Apply the LLM judge\n",
    "eval_dataset = Dataset.from_pandas(\n",
    "    pd.DataFrame(review_dataset),\n",
    "    data_definition=definition,\n",
    "    descriptors=[\n",
    "        LLMEval(\"Generated review\",\n",
    "                template=feedback_quality_2,# We can pass new prompt version <- new prompt\n",
    "                provider=\"openai\",          # We can change the provider\n",
    "                model=\"gpt-4o-mini\",        # We can change the model\n",
    "                alias=\"LLM-judged quality\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 4. Add TRUE/FALSE for judge alignment\n",
    "eval_dataset.add_descriptors([\n",
    "    ExactMatch(columns=[\"LLM-judged quality\", \"Expert label\"], alias=\"Judge_alignment\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-mahKjhusIP"
   },
   "source": [
    "Evaluate the LLM judge quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NMflTRZWuurK"
   },
   "outputs": [],
   "source": [
    "my_eval = run_classification_report(\n",
    "    eval_dataset,\n",
    "    name=name,\n",
    "    cloud_ws=ws, #Optional\n",
    "    project_id=project.id #Optional\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M19OtYbSVxOF"
   },
   "source": [
    "# Exp 3. Can we make it better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rb9MkrcpwRhz"
   },
   "outputs": [],
   "source": [
    "# 1. Name the experiment <- new name\n",
    "name = \"detailed_prompt_think_better\"\n",
    "\n",
    "# 2. Define LLM judge prompt template  <- new prompt\n",
    "feedback_quality_3 = BinaryClassificationPromptTemplate(\n",
    "    pre_messages=[\n",
    "        (\"system\", \"You are evaluating the quality of code reviews given to junior developers.\")],\n",
    "    criteria=\"\"\"\n",
    "    A review is **GOOD** if it is actionable and constructive. It should:\n",
    "    - Offer clear, specific suggestions or highlight issues in a way that the developer can address\n",
    "    - Be respectful and encourage learning or improvement\n",
    "    - Use professional, helpful language—even when pointing out problems\n",
    "\n",
    "    A review is **BAD** if it is non-actionable or overly critical. For example:\n",
    "    - It may be vague, generic, or hedged to the point of being unhelpful\n",
    "    - It may focus on praise only, without offering guidance\n",
    "    - It may sound dismissive, contradictory, harsh, or robotic\n",
    "    - It may raise a concern but fail to explain what should be done\n",
    "\n",
    "    Always explain your reasoning.\n",
    "    \"\"\",\n",
    "    target_category=\"bad\",\n",
    "    non_target_category=\"good\",\n",
    "    uncertainty=\"unknown\",\n",
    "    include_reasoning=True,\n",
    ")\n",
    "\n",
    "# 3. Apply the LLM judge\n",
    "eval_dataset = Dataset.from_pandas(\n",
    "    pd.DataFrame(review_dataset),\n",
    "    data_definition=definition,\n",
    "    descriptors=[\n",
    "        LLMEval(\"Generated review\",\n",
    "                template=feedback_quality_3,# We can pass new prompt version  <- new prompt\n",
    "                provider=\"openai\",          # We can change the provider\n",
    "                model=\"gpt-4o-mini\",        # We can change the model\n",
    "                alias=\"LLM-judged quality\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 4. Add TRUE/FALSE for judge alignment\n",
    "eval_dataset.add_descriptors([\n",
    "    ExactMatch(columns=[\"LLM-judged quality\", \"Expert label\"], alias=\"Judge_alignment\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JBCDLhU_witm"
   },
   "outputs": [],
   "source": [
    "my_eval = run_classification_report(\n",
    "    eval_dataset,\n",
    "    name=name,\n",
    "    cloud_ws=ws, #Optional\n",
    "    project_id=project.id #Optional\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qnGApwNbwtiY"
   },
   "outputs": [],
   "source": [
    "#my_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xf3OsJHQV5-x"
   },
   "source": [
    "# Exp 4. Try a different model (Turbo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfuJY18KxlH-"
   },
   "source": [
    "Can a cheaper, simpler model perform as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2EEEvf7MxjPS"
   },
   "outputs": [],
   "source": [
    "# 1. Name the experiment  <- new name\n",
    "name = \"turbo\"\n",
    "\n",
    "# 2. Define LLM judge prompt template\n",
    "feedback_quality_3 = BinaryClassificationPromptTemplate(\n",
    "    pre_messages=[\n",
    "        (\"system\", \"You are evaluating the quality of code reviews given to junior developers.\")],\n",
    "    criteria=\"\"\"\n",
    "    A review is **GOOD** if it is actionable and constructive. It should:\n",
    "    - Offer clear, specific suggestions or highlight issues in a way that the developer can address\n",
    "    - Be respectful and encourage learning or improvement\n",
    "    - Use professional, helpful language—even when pointing out problems\n",
    "\n",
    "    A review is **BAD** if it is non-actionable or overly critical. For example:\n",
    "    - It may be vague, generic, or hedged to the point of being unhelpful\n",
    "    - It may focus on praise only, without offering guidance\n",
    "    - It may sound dismissive, contradictory, harsh, or robotic\n",
    "    - It may raise a concern but fail to explain what should be done\n",
    "\n",
    "    Always explain your reasoning.\n",
    "    \"\"\",\n",
    "    target_category=\"bad\",\n",
    "    non_target_category=\"good\",\n",
    "    uncertainty=\"unknown\",\n",
    "    include_reasoning=True,\n",
    ")\n",
    "\n",
    "# 3. Apply the LLM judge\n",
    "eval_dataset = Dataset.from_pandas(\n",
    "    pd.DataFrame(review_dataset),\n",
    "    data_definition=definition,\n",
    "    descriptors=[\n",
    "        LLMEval(\"Generated review\",\n",
    "                template=feedback_quality_3,  # We can pass new prompt version\n",
    "                provider=\"openai\",            # We can change the provider\n",
    "                model=\"gpt-3.5-turbo\",        # We can change the model  <- different model\n",
    "                alias=\"LLM-judged quality\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 4. Add TRUE/FALSE for judge alignment\n",
    "eval_dataset.add_descriptors([\n",
    "    ExactMatch(columns=[\"LLM-judged quality\", \"Expert label\"], alias=\"Judge_alignment\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZI3hV2MyV1J"
   },
   "outputs": [],
   "source": [
    "my_eval = run_classification_report(\n",
    "    eval_dataset,\n",
    "    name=name,\n",
    "    cloud_ws=ws, #Optional\n",
    "    project_id=project.id #Optional\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwEcCT4xWOgm"
   },
   "source": [
    "# Exp 5. Try another provider (Anthropic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8OAM3KLJW8B4"
   },
   "outputs": [],
   "source": [
    "from evidently.legacy.utils.llm.wrapper import AnthropicOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_UkMiAAzy5x4"
   },
   "outputs": [],
   "source": [
    "# 1. Name the experiment <- new name\n",
    "name = \"anthropic\"\n",
    "\n",
    "# 2. Define LLM judge prompt template\n",
    "feedback_quality_3 = BinaryClassificationPromptTemplate(\n",
    "    pre_messages=[\n",
    "        (\"system\", \"You are evaluating the quality of code reviews given to junior developers.\")],\n",
    "    criteria=\"\"\"\n",
    "    A review is **GOOD** if it is actionable and constructive. It should:\n",
    "    - Offer clear, specific suggestions or highlight issues in a way that the developer can address\n",
    "    - Be respectful and encourage learning or improvement\n",
    "    - Use professional, helpful language—even when pointing out problems\n",
    "\n",
    "    A review is **BAD** if it is non-actionable or overly critical. For example:\n",
    "    - It may be vague, generic, or hedged to the point of being unhelpful\n",
    "    - It may focus on praise only, without offering guidance\n",
    "    - It may sound dismissive, contradictory, harsh, or robotic\n",
    "    - It may raise a concern but fail to explain what should be done\n",
    "\n",
    "    Always explain your reasoning.\n",
    "    \"\"\",\n",
    "    target_category=\"bad\",\n",
    "    non_target_category=\"good\",\n",
    "    uncertainty=\"unknown\",\n",
    "    include_reasoning=True,\n",
    ")\n",
    "\n",
    "# 3. Apply the LLM judge\n",
    "eval_dataset = Dataset.from_pandas(\n",
    "    pd.DataFrame(review_dataset),\n",
    "    data_definition=definition,\n",
    "    descriptors=[\n",
    "        LLMEval(\"Generated review\",\n",
    "                template=feedback_quality_3,         # We can pass new prompt version\n",
    "                provider=\"anthropic\",                # We can change the provider <- new provider\n",
    "                model=\"claude-3-5-sonnet-20240620\",  # We can change the model <- new model\n",
    "                alias=\"LLM-judged quality\")\n",
    "    ],\n",
    "    options=AnthropicOptions(rpm_limit=50)  # <- rate limit params\n",
    ")\n",
    "\n",
    "# 4. Add TRUE/FALSE for judge alignment\n",
    "eval_dataset.add_descriptors([\n",
    "    ExactMatch(columns=[\"LLM-judged quality\", \"Expert label\"], alias=\"Judge_alignment\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evKn7rAOzQsh"
   },
   "outputs": [],
   "source": [
    "my_eval = run_classification_report(\n",
    "    eval_dataset,\n",
    "    name=name,\n",
    "    cloud_ws=ws, #Optional\n",
    "    project_id=project.id #Optional\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85f045lnC_xy"
   },
   "source": [
    "# What's next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3r1rbowDBl8"
   },
   "source": [
    "Can you make a better prompt? Conside splitting the criteria - and review the labels first! (Even humans don't always agree :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lx-s7J72DSF1"
   },
   "source": [
    "Enjoyed the tutorial? Star Evidently on GitHub to support the project: https://github.com/evidentlyai/evidently"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
